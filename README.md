ğŸ§© PHASE 1 â€” TEXT RAG (RECAP DIAGRAM)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Wikipedia Pages   â”‚
â”‚  (~100 articles)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ 1. Fetch content (wikipedia API)
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Text Content  â”‚
â”‚  (long articles)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ 2. Chunking
          â”‚    (RecursiveCharacterTextSplitter)
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text Chunks       â”‚
â”‚  (~12,500 chunks) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ 3. Embeddings
          â”‚    (Sentence Transformers)
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector Embeddings (384-dim)   â”‚
â”‚  all-MiniLM-L6-v2              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ 4. Store vectors + metadata
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  QDRANT Vector Database        â”‚
â”‚  - embedding                  â”‚
â”‚  - title                      â”‚
â”‚  - source (URL)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ 5. User Query
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Question     â”‚
â”‚ "What is DL?"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ 6. Query embedding
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Vector                  â”‚
â”‚  (same embedding model)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ 7. Similarity Search
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Top-K Chunks                  â”‚
â”‚  (semantic matches)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ 8. Output
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Retrieved Text + Sources      â”‚
â”‚  (Wikipedia links)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§  PHASE 1 â€” FLOW EXPLAINED IN SIMPLE WORDS
1ï¸âƒ£ Data ingestion

You pulled Wikipedia articles using the wikipedia Python library.

2ï¸âƒ£ Chunking (why this matters)

LLMs and embedding models:

Cannot handle very long text

Work best on 500â€“1,000 token chunks

So you split each article into:

[ chunk 1 ][ chunk 2 ][ chunk 3 ] ...


This allows:

Fine-grained retrieval

Better semantic matching

3ï¸âƒ£ Embeddings (core idea)

Each chunk is converted into a 384-dimensional vector.

Key rule:

Text with similar meaning â†’ vectors close together

This is why:

â€œWhat is deep learning?â€

â€œExplain deep neural networksâ€

â†’ retrieve the same content.

4ï¸âƒ£ Vector storage (Qdrant)

Qdrant stores:

Vector

Metadata (title, source URL)

It does fast cosine similarity search.

5ï¸âƒ£ Query-time flow

When you ask a question:

Query is embedded

Vector is compared against 12,560 stored vectors

Top-K closest matches returned


ğŸ¯ PHASE 1 â€” WHAT YOU ACHIEVED (IMPORTANT)

You now understand and built:

âœ… RAG fundamentals
âœ… Vector databases
âœ… Chunking strategy
âœ… Embedding models
âœ… Semantic retrieval
âœ… Source attribution

This is exactly the same foundation used by:

ChatGPT Retrieval

Perplexity-style search

Internal enterprise RAG systems

ğŸ”® HOW PHASE 2 BUILDS ON THIS

Phase 2 will reuse everything above and add:

Text Embeddings  â”€â”
                  â”œâ”€â”€â–º Unified Vector Space (CLIP)
Image Embeddings â”€â”˜


Meaning:

Text â†’ image search

Image â†’ text search

Multimodal chat later
